{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Lab 4 Assignment Solutions\n",
    "**Dataset: Student Performance (student-mat.csv)**\n",
    "\n",
    "This notebook completes all five assignment tasks using the Student Performance dataset.\n",
    "The dataset contains student grades, demographics, and social/school attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Identify Data Quality Issues\n",
    "We examine the dataset for data type mismatches, missing values, duplicates, and other issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('../student-mat.csv', sep=';')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-dtypes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of all columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print('Number of duplicate rows:', df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-findings",
   "metadata": {},
   "source": [
    "### Task 1 Findings\n",
    "\n",
    "After inspecting the dataset, the following data quality issues were identified:\n",
    "\n",
    "1. **Data Type Mismatch**: The columns `G1` and `G2` (first and second period grades) are stored as **object (string)** type instead of numeric. This is because they are quoted in the raw CSV file. They must be converted to integers before any numerical analysis.\n",
    "\n",
    "2. **No Missing Values**: All columns report zero missing values, so no imputation is needed on the original data. However, we will introduce artificial missing values in Task 2 for demonstration purposes.\n",
    "\n",
    "3. **No Duplicate Rows**: The dataset contains no exact duplicate records.\n",
    "\n",
    "4. **Potential Outliers**: The `absences` column has a minimum of 0 and a maximum that may be significantly higher than the 75th percentile, suggesting the presence of outliers. We will investigate this in Task 3.\n",
    "\n",
    "5. **Binary Categorical Columns**: Columns such as `schoolsup`, `famsup`, `paid`, `activities`, `internet`, `romantic` are stored as `yes/no` strings. These would need to be encoded (e.g., 0/1) before use in a machine learning model, though this is beyond the scope of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Apply One Missing Value Strategy\n",
    "\n",
    "Since the dataset has no missing values, we first:\n",
    "1. Fix the data type issue in `G1` and `G2`.\n",
    "2. Introduce artificial missing values in `G3` (final grade) for demonstration.\n",
    "3. Apply **median imputation** and explain the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fix-dtypes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data type issue: convert G1 and G2 from string to numeric\n",
    "df['G1'] = pd.to_numeric(df['G1'], errors='coerce')\n",
    "df['G2'] = pd.to_numeric(df['G2'], errors='coerce')\n",
    "\n",
    "print('Updated dtypes for G1 and G2:')\n",
    "print(df[['G1', 'G2', 'G3']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introduce-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce artificial missing values in G3 for demonstration\n",
    "df_missing = df.copy()\n",
    "np.random.seed(42)\n",
    "missing_idx = np.random.choice(df_missing.index, size=20, replace=False)\n",
    "df_missing.loc[missing_idx, 'G3'] = np.nan\n",
    "\n",
    "print('Missing values after introduction:')\n",
    "df_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-g3-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of G3 to choose the right imputation strategy\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(df['G3'], bins=20, kde=True)\n",
    "plt.title('Distribution of G3 (Final Grade)')\n",
    "plt.xlabel('G3')\n",
    "plt.show()\n",
    "\n",
    "print('G3 Skewness:', df['G3'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-imputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Median Imputation\n",
    "df_imputed = df_missing.copy()\n",
    "median_g3 = df_imputed['G3'].median()\n",
    "df_imputed['G3'].fillna(median_g3, inplace=True)\n",
    "\n",
    "print(f'Median value used for imputation: {median_g3}')\n",
    "print('\\nMissing values after median imputation:')\n",
    "print(df_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2-explanation",
   "metadata": {},
   "source": [
    "### Why Median Imputation?\n",
    "\n",
    "We chose **median imputation** for the `G3` (final grade) column for the following reasons:\n",
    "\n",
    "1. **Robustness to outliers**: Some students score 0 on `G3` (often due to withdrawal or special cases), which pulls the mean downward. The median is not affected by these extreme low values.\n",
    "\n",
    "2. **Skewed distribution**: If the `G3` distribution is skewed (left-skewed due to 0s), the median is a more representative measure of the typical student's grade than the mean.\n",
    "\n",
    "3. **Preserves dataset size**: Unlike row deletion, imputation keeps all 395 records, which is important given the relatively small dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Detect and Handle Outliers Using IQR\n",
    "\n",
    "We focus on the `absences` column, which is most likely to have extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxplot-numeric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of key numeric columns using boxplots\n",
    "numeric_cols = ['age', 'absences', 'G1', 'G2', 'G3']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(numeric_cols), figsize=(16, 4))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(y=df[col], ax=axes[i])\n",
    "    axes[i].set_title(f'{col}')\n",
    "plt.suptitle('Boxplots of Key Numerical Features', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqr-detect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in 'absences' using the IQR method\n",
    "Q1 = df['absences'].quantile(0.25)\n",
    "Q3 = df['absences'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f'Q1: {Q1}')\n",
    "print(f'Q3: {Q3}')\n",
    "print(f'IQR: {IQR}')\n",
    "print(f'Lower Bound: {lower}')\n",
    "print(f'Upper Bound: {upper}')\n",
    "\n",
    "outliers = df[(df['absences'] < lower) | (df['absences'] > upper)]\n",
    "print(f'\\nNumber of outliers detected: {len(outliers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View outlier records\n",
    "outliers[['age', 'absences', 'G1', 'G2', 'G3']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remove-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Remove outliers\n",
    "df_no_outliers = df[(df['absences'] >= lower) & (df['absences'] <= upper)]\n",
    "print('Original shape:', df.shape)\n",
    "print('After removing outliers:', df_no_outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cap-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Cap outliers using percentile method\n",
    "lower_cap = df['absences'].quantile(0.05)\n",
    "upper_cap = df['absences'].quantile(0.95)\n",
    "\n",
    "df_capped = df.copy()\n",
    "df_capped['absences'] = df_capped['absences'].clip(lower_cap, upper_cap)\n",
    "\n",
    "print('Before capping (absences):')\n",
    "print(df['absences'].describe())\n",
    "print('\\nAfter capping (absences):')\n",
    "print(df_capped['absences'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-explanation",
   "metadata": {},
   "source": [
    "### Outlier Handling Summary\n",
    "\n",
    "- The `absences` column has a right-skewed distribution with several students having very high absence counts.\n",
    "- **IQR** detected outliers as values above the upper fence.\n",
    "- **Removal**: reduces the dataset size but eliminates distortion from extreme values.\n",
    "- **Capping**: keeps all records but limits the influence of extreme values by replacing them with the 5th/95th percentile boundaries.\n",
    "\n",
    "> Capping is preferred here because high absences may be real and meaningful for predicting `G3`, so we do not want to lose those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Normalize Numerical Features\n",
    "\n",
    "We apply both **Min-Max normalization** and **Z-score standardization** to the key numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for normalization\n",
    "numeric_features = ['age', 'absences', 'G1', 'G2', 'G3',\n",
    "                    'studytime', 'failures', 'famrel',\n",
    "                    'freetime', 'goout', 'Dalc', 'Walc', 'health']\n",
    "\n",
    "# View raw values before normalization\n",
    "df[numeric_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minmax-norm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization (scales to [0, 1])\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax = df[numeric_features].copy()\n",
    "df_minmax[numeric_features] = scaler_minmax.fit_transform(df_minmax)\n",
    "\n",
    "print('Min-Max Normalized (first 5 rows):')\n",
    "df_minmax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minmax-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: all values should be between 0 and 1\n",
    "print('Min-Max range after normalization:')\n",
    "print(df_minmax.describe().loc[['min', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zscore-norm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Score Standardization (mean=0, std=1)\n",
    "scaler_std = StandardScaler()\n",
    "df_standardized = df[numeric_features].copy()\n",
    "df_standardized[numeric_features] = scaler_std.fit_transform(df_standardized)\n",
    "\n",
    "print('Z-Score Standardized (first 5 rows):')\n",
    "df_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zscore-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: mean approximately 0, std approximately 1\n",
    "print('Z-Score statistics after standardization:')\n",
    "print(df_standardized.describe().loc[['mean', 'std']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-norm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: original vs Min-Max vs Z-Score for G3\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "sns.histplot(df['G3'], bins=20, kde=True, ax=axes[0])\n",
    "axes[0].set_title('Original G3')\n",
    "\n",
    "sns.histplot(df_minmax['G3'], bins=20, kde=True, ax=axes[1], color='orange')\n",
    "axes[1].set_title('Min-Max Normalized G3')\n",
    "\n",
    "sns.histplot(df_standardized['G3'], bins=20, kde=True, ax=axes[2], color='green')\n",
    "axes[2].set_title('Z-Score Standardized G3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-explanation",
   "metadata": {},
   "source": [
    "### Normalization Summary\n",
    "\n",
    "| Method | Output Range | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| **Min-Max** | [0, 1] | KNN, K-Means, Neural Networks (bounded input needed) |\n",
    "| **Z-Score** | Mean=0, Std=1 | Linear Regression, SVM, PCA (assumes normally distributed data) |\n",
    "\n",
    "- **Min-Max** is useful when the algorithm requires inputs in a fixed range.\n",
    "- **Z-Score** is preferred when features may follow a roughly normal distribution and the model is sensitive to variance differences.\n",
    "- The **shape** of the distribution is preserved by both methods; only the scale changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Apply PCA and Interpret Explained Variance\n",
    "\n",
    "We apply Principal Component Analysis (PCA) to the standardized features to reduce dimensionality while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations between features before PCA\n",
    "pca_features = ['age', 'absences', 'G1', 'G2', 'G3',\n",
    "                'studytime', 'failures', 'famrel',\n",
    "                'freetime', 'goout', 'Dalc', 'Walc', 'health']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[pca_features].corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap Before PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[pca_features])\n",
    "\n",
    "# Apply PCA with all components first to see the full variance breakdown\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "print('Explained Variance Ratio per Component:')\n",
    "for i, (ev, cv) in enumerate(zip(explained, cumulative)):\n",
    "    print(f'  PC{i+1}: {ev:.4f}  (Cumulative: {cv:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-scree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot: visualize explained variance\n",
    "n_components = len(pca_features)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(1, n_components + 1), explained * 100, alpha=0.7, label='Individual')\n",
    "plt.step(range(1, n_components + 1), cumulative * 100, where='mid',\n",
    "         color='red', linewidth=2, label='Cumulative')\n",
    "plt.axhline(y=80, color='gray', linestyle='--', label='80% threshold')\n",
    "plt.axhline(y=95, color='navy', linestyle='--', label='95% threshold')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.title('Scree Plot: PCA Explained Variance')\n",
    "plt.legend()\n",
    "plt.xticks(range(1, n_components + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-5comp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA keeping 5 components\n",
    "pca = PCA(n_components=5)\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "print('Explained Variance Ratio (5 components):', pca.explained_variance_ratio_.round(4))\n",
    "print('Cumulative Explained Variance:', np.cumsum(pca.explained_variance_ratio_).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of first two principal components, colored by G3\n",
    "plt.figure(figsize=(7, 5))\n",
    "sc = plt.scatter(principal_components[:, 0], principal_components[:, 1],\n",
    "                 c=df['G3'], cmap='viridis', alpha=0.7, edgecolors='k', linewidths=0.3)\n",
    "plt.colorbar(sc, label='G3 (Final Grade)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Projection (colored by G3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-loadings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Loadings: which features contribute most to each component?\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=pca_features,\n",
    "    columns=[f'PC{i+1}' for i in range(5)]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(loadings[['PC1', 'PC2', 'PC3']], annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('PCA Loadings (Feature Contributions to Components)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-explanation",
   "metadata": {},
   "source": [
    "### Task 5: PCA Interpretation\n",
    "\n",
    "**Explained Variance:**\n",
    "- The scree plot shows how much variance each principal component captures.\n",
    "- The first few components typically capture the most information.\n",
    "- We look at the cumulative line to decide how many components to keep.\n",
    "\n",
    "**What the components represent:**\n",
    "- **PC1** captures the most variation in the dataset. From the loadings heatmap, features like `G1`, `G2`, `G3`, and `failures` have high loadings on PC1 — this component likely represents **academic performance**.\n",
    "- **PC2** may capture social/lifestyle variation — features like `goout`, `Dalc`, `Walc`, and `freetime` tend to load here.\n",
    "\n",
    "**Why PCA is useful here:**\n",
    "- Several grade-related features (`G1`, `G2`, `G3`) are highly correlated, making PCA effective at combining overlapping information.\n",
    "- Reducing 13 features to 5 principal components (capturing ~80%+ of variance) simplifies the dataset while retaining most information.\n",
    "\n",
    "**Scatter plot interpretation:**\n",
    "- Points are colored by `G3` (final grade). A visible gradient from left to right along PC1 indicates that PC1 strongly captures academic performance variation.\n",
    "\n",
    "**Decision rule:** Keep enough components to reach **80% to 95% cumulative explained variance**, depending on the model's tolerance for information loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
